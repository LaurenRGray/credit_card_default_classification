{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from patsy import dmatrices, dmatrix\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "%matplotlib inline\n",
    "\n",
    "# make prettier plots\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cleaned_cc_default_data', 'rb')\n",
    "model_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train/test & scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Patsy to create my X Matrix\n",
    "\n",
    "## QUESTION: need to change variable names to avoid mix up with non clustered variables above?\n",
    "\n",
    "x_cols = ['my_clusters',\n",
    "        'age', 'sex', 'marital_status', 'education_level',  \n",
    "        'bill_amt_1', 'bill_amt_2', 'bill_amt_3', 'bill_amt_4', 'bill_amt_5', 'bill_amt_6', \n",
    "        'pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6', \n",
    "        'pay_amt_1', 'pay_amt_2', 'pay_amt_3', 'pay_amt_4', 'pay_amt_5', 'pay_amt_6', \n",
    "         'limit_balance']\n",
    "# Add interactions\n",
    "x_str = x_cols[0]\n",
    "for i in x_cols[1:]:\n",
    "    x_str = x_str + ' + ' + i\n",
    "x_str = x_str + '' # if I want to create new variables, add in empty string\n",
    "\n",
    "x_patsy = dmatrix(x_str, model_data)\n",
    "x_patsy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train test split\n",
    "\n",
    "# CHANGE VARIABLE NAMES?\n",
    "\n",
    "x_raw_clusters = x_patsy\n",
    "y_raw_clusters = np.array(model_data['default_payment_next_month'])\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "# sss2 = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n",
    "\n",
    "sss1.get_n_splits()\n",
    "for train_index, test_index in sss1.split(x_raw, y_raw):\n",
    "    x_train_clusters, x_test_clusters = x_raw_clusters[train_index,:], x_raw_clusters[test_index,:]\n",
    "    y_train_clusters, y_test_clusters = y_raw_clusters[train_index], y_raw_clusters[test_index]\n",
    "\n",
    "# use this later if I want to get fancy...\n",
    "# sss2.get_n_splits()\n",
    "# for train_index, test_index in sss2.split(x_mid, y_mid):\n",
    "#     x_train, x_val = x_mid[train_index,:], x_mid[test_index,:]\n",
    "#     y_train, y_val = y_mid[train_index], y_mid[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize my variables\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_norm_train = scaler.transform(x_train)\n",
    "# x_norm_val = scaler.transform(x_val)\n",
    "x_norm_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: once I've picked my model, start doing EDA to understand how the top features are contributing to the model (e.g., do married people tend to default less often on their pyaments?)\n",
    "# do this for the top 10 features for Random Forest or Gradient Boosting Trees (or XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
