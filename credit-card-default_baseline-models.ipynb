{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from patsy import dmatrices, dmatrix\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "%matplotlib inline\n",
    "\n",
    "# make prettier plots\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cleaned_cc_default_data', 'rb')\n",
    "model_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into train/test & scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Patsy to create my X Matrix\n",
    "x_cols = ['age', 'sex', 'marital_status', 'education_level',  \n",
    "        'bill_amt_1', 'bill_amt_2', 'bill_amt_3', 'bill_amt_4', 'bill_amt_5', 'bill_amt_6', \n",
    "        'pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6', \n",
    "        'pay_amt_1', 'pay_amt_2', 'pay_amt_3', 'pay_amt_4', 'pay_amt_5', 'pay_amt_6', \n",
    "         'limit_balance']\n",
    "\n",
    "# # Add interactions\n",
    "x_str = x_cols[0]\n",
    "for i in x_cols[1:]:\n",
    "    x_str = x_str + ' + ' + i\n",
    "x_str = x_str + '' # if I want to create new variables, add in empty string\n",
    "\n",
    "x_patsy = dmatrix(x_str, model_data)\n",
    "x_patsy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and then train/test split of my variables\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# x_raw = x_patsy\n",
    "# y_raw = model_data['outcome_var']\n",
    "\n",
    "#x_shuffle, y_shuffle = shuffle(x_raw, y_raw)\n",
    "#x_mid, x_test, y_mid, y_test = train_test_split(x_shuffle, y_shuffle, test_size=0.2, random_state=0)\n",
    "#x_train, x_val, y_train, y_val = train_test_split(x_mid, y_mid, test_size=0.25, random_state=0)\n",
    "\n",
    "# Stratified train test split\n",
    "x_raw = x_patsy\n",
    "y_raw = np.array(model_data['default_payment_next_month'])\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "# sss2 = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n",
    "\n",
    "sss1.get_n_splits()\n",
    "for train_index, test_index in sss1.split(x_raw, y_raw):\n",
    "    x_train, x_test = x_raw[train_index,:], x_raw[test_index,:]\n",
    "    y_train, y_test = y_raw[train_index], y_raw[test_index]\n",
    "    \n",
    "# use this later if I want to get fancy...\n",
    "# sss2.get_n_splits()\n",
    "# for train_index, test_index in sss2.split(x_mid, y_mid):\n",
    "#     x_train, x_val = x_mid[train_index,:], x_mid[test_index,:]\n",
    "#     y_train, y_val = y_mid[train_index], y_mid[test_index]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_cols = model_data[['age', 'sex', 'marital_status', 'education_level',  \n",
    "        'bill_amt_1', 'bill_amt_2', 'bill_amt_3', 'bill_amt_4', 'bill_amt_5', 'bill_amt_6', \n",
    "        'pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6', \n",
    "        'pay_amt_1', 'pay_amt_2', 'pay_amt_3', 'pay_amt_4', 'pay_amt_5', 'pay_amt_6', \n",
    "         'limit_balance']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_cols = model_data.loc[:, ['age', 'sex', 'marital_status', 'education_level',  \n",
    "        'bill_amt_1', 'bill_amt_2', 'bill_amt_3', 'bill_amt_4', 'bill_amt_5', 'bill_amt_6', \n",
    "        'pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6', \n",
    "        'pay_amt_1', 'pay_amt_2', 'pay_amt_3', 'pay_amt_4', 'pay_amt_5', 'pay_amt_6', \n",
    "         'limit_balance']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_raw.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stratified train test split\n",
    "x_raw = x_cols\n",
    "y_raw = model_data['default_payment_next_month']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_raw, y_raw, test_size=0.2,random_state=42)\n",
    "\n",
    "# sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "# # sss2 = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n",
    "\n",
    "# sss1.get_n_splits()\n",
    "# for train_index, test_index in sss1.split(x_raw, y_raw):\n",
    "#     x_train, x_test = x_raw[train_index,:], x_raw[test_index,:]\n",
    "#     y_train, y_test = y_raw[train_index], y_raw[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train data into features that will be scaled\n",
    "# by dropping categorical features\n",
    "\n",
    "X_for_scaling = X_train.drop(columns=['Intercept', 'RandD', 'accounting', 'hr',\n",
    "       'management', 'marketing', 'product_mng', 'support', 'Work_accident',\n",
    "       'promotion_last_5years', 'technical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features that need to be scaled\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_for_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert those features to data frame\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge scaled features with non-scaled features\n",
    "\n",
    "X_train = pd.merge(X_train_scaled, X_train.drop(columns=['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "       'average_monthly_hours', 'time_spend_company', 'salary', 'int_term_1']).reset_index(drop=True), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test data into features that will be scaled by dropping categorical features (same process as train data)\n",
    "\n",
    "X_for_scaling2 = X_test.drop(columns=['Intercept', 'RandD', 'accounting', 'hr',\n",
    "       'management', 'marketing', 'product_mng', 'support', 'Work_accident',\n",
    "       'promotion_last_5years', 'technical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test features\n",
    "\n",
    "X_test_scaled = scaler.fit_transform(X_for_scaling2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert those features to dataframe\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with categorical features that weren't scaled\n",
    "\n",
    "X_test = pd.merge(X_test_scaled, X_test.drop(columns=['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "       'average_monthly_hours', 'time_spend_company', 'salary', 'int_term_1']).reset_index(drop=True), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Normalize my variables\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_norm_train = scaler.transform(x_train)\n",
    "# x_norm_val = scaler.transform(x_val)\n",
    "x_norm_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Round I\n",
    "\n",
    "- KNN\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Linear SVC\n",
    "- Naive Bayes\n",
    "- Decision Tree Classifier\n",
    "- Random Forest\n",
    "- XGBoost ?\n",
    "- LightGBM ?\n",
    "- Neural Net ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing confusion matrices (see: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823)\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=18):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names, )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (knn)\n",
    "\n",
    "ks = range(1,301,50)\n",
    "param_grid = [{'n_neighbors': ks}]\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(knn, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "knn_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my best model based on above\n",
    "# can run .predict(x_test)\n",
    "\n",
    "knn_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, knn_grid.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTES\n",
    "\n",
    "# need to do np.exp(coefficients) to scale them back from log odds after you get coefficients\n",
    "# also need to unscale (because originally scaled them)\n",
    "# note: no need to scale categorical variables\n",
    "# create a subset of dataframe; scale it; drop it back in\n",
    "# for pay_# columns -- use label encoding, because you want to keep it valued as it is; do not scale either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (logit)\n",
    "\n",
    "penalty = ['l1', 'l2'] # look into doing elastic net here to see what combo of l1 and l2 is best\n",
    "C = np.logspace(-3, 1, 100)\n",
    "param_grid = dict(C=C, penalty=penalty)\n",
    "\n",
    "logistic = linear_model.LogisticRegression(solver='liblinear', max_iter=10000)\n",
    "logistic_grid = GridSearchCV(logistic, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "logistic_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model for logistic regression (metric = C)\n",
    "\n",
    "logistic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, logistic_grid.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Lee's code for feature importance for logistic regression\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coefficients\n",
    "# then get coefficients\n",
    "# those coefficients will tell me which features are most important\n",
    "# purpose of this is interpretation of what really contributed to my model (presentation)\n",
    "\n",
    "logistic2 = linear_model.LogisticRegression(C=0.02848035868435802, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
    "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False) # ADD PARAMETERS FROM BEST ESTIMATE\n",
    "logistic2.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = logistic2.coef_\n",
    "# a_list = a.tolist()\n",
    "# flat_list = [item for x in a_list for item in x]\n",
    "# a_df = pd.DataFrame(flat_list)\n",
    "# a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = x_cols\n",
    "# b_df = pd.DataFrame(b)\n",
    "# b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_coef = pd.concat([b_df, a_df], ignore_index = True, axis = 1)\n",
    "# logistic_coef\n",
    "\n",
    "list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because ROC-AUC scores between logit and RF are so close, can just opt to use logit since it will get rid of features (becuase it chose l1)\n",
    "# or if I decide to use RF, can then do EDA on features of low importance to inuit which features are important vs. not and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (SVM)\n",
    "\n",
    "C = np.logspace(-3, 1, 25)\n",
    "gammas = np.logspace(-3, 0, 25)\n",
    "param_grid = dict(C=C, gamma=gammas)\n",
    "\n",
    "svm1 = svm.SVC(kernel='rbf', probability=True)\n",
    "svm_grid = GridSearchCV(svm1, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "svm_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, svm_grid.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC\n",
    "\n",
    "IS THE CODE CORRECT?\n",
    "\n",
    "HOW TO ADD TO ROC/AUC/SCORING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (Linear SVC)\n",
    "\n",
    "C = np.logspace(-3, 1, 25)\n",
    "# gammas = np.logspace(-3, 0, 25)\n",
    "param_grid = dict(C=C)\n",
    "# param_grid = dict(C=C, gamma=gammas)\n",
    "\n",
    "\n",
    "# LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "#      multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
    "\n",
    "svc1 = LinearSVC()\n",
    "svc_grid = GridSearchCV(svc1, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "svc_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, svc_grid.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Naive Bayes Model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb_best = gnb.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, gnb_best.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier GridSearchCV\n",
    "\n",
    "CORRECT CODE BELOW?\n",
    "\n",
    "ADD TO ROC-AUC & SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (Decision Tree Classifier)\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [50, 100, None],\n",
    "    'max_features': ['sqrt'], # what is this?\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'min_samples_split': [2, 3, 5, 10],\n",
    "    'n_estimators': [100, 200, 400, 1000]\n",
    "}\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree_grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "dectree_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, dectree_grid.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (Random Forest)\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [50, 100, None],\n",
    "    'max_features': ['sqrt'], # what is this?\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'min_samples_split': [2, 3, 5, 10],\n",
    "    'n_estimators': [100, 200, 400, 1000]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "rf_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_train, rf_grid.predict(x_norm_train)), ['Class 0', 'Class 1'], figsize=(5, 4), fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Feature Importances\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators=200, max_depth = None)\n",
    "rf2.fit(x_norm_train, y_train)\n",
    "rf2.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip each feature importance weight with my columns\n",
    "\n",
    "pd.DataFrame(zip(list(rf2.feature_importances_), model_data.columns)) # sort by 0 later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid={\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.05, 0.075, 0.1],\n",
    "    \"min_samples_split\": np.linspace(0.01, 0.1, 6),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 6),\n",
    "    \"max_depth\":[10,20],\n",
    "    \"max_features\":[\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[ 1.0],\n",
    "    \"n_estimators\":[125]\n",
    "    }\n",
    "gbc_grid = GridSearchCV(GradientBoostingClassifier(), parameter_grid, cv=5, n_jobs=-1,scoring = 'roc_auc', verbose= True)\n",
    "gbc_grid.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run later and do feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best ROC_AUC for knn: %0.4f' % knn_grid.best_score_)\n",
    "print('Best ROC_AUC for logit: %0.4f' % logistic_grid.best_score_)\n",
    "print('Best ROC_AUC for svm: %0.4f' % svm_grid.best_score_)\n",
    "print('Best ROC_AUC for rf: %0.4f' % rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on F1\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# y_true = \n",
    "# y_pred = \n",
    "\n",
    "# f1_score(y_true, y_pred, average='macro')  \n",
    "\n",
    "# f1_score(y_true, y_pred, average='micro')  \n",
    "\n",
    "# f1_score(y_true, y_pred, average='weighted')  \n",
    "\n",
    "# f1_score(y_true, y_pred, average=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Params for knn: ', knn_grid.best_params_)\n",
    "print('Best Patams for logit: ', logistic_grid.best_params_)\n",
    "print('Best Params for svm: ', svm_grid.best_params_)\n",
    "print('Best Params for rf: ', rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Fit Dummy Classifier\n",
    "\n",
    "# knowing class split, makes educated guess\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(x_norm_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE QUESTIONS IN BELOW COMMENTED CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC for all the models\n",
    "\n",
    "# ARE THE X/Y VARIABLES THE CORRECT ONES TO USE HERE?\n",
    "# How does the ensembe work?\n",
    "\n",
    "# How to add new models above into this? E.g., Linear SVC, \n",
    "\n",
    "model_list = [knn_grid.best_estimator_, \n",
    "              logistic_grid.best_estimator_, \n",
    "              svm_grid.best_estimator_, \n",
    "              gnb_best, \n",
    "              rf_grid.best_estimator_,\n",
    "              'ensemble']\n",
    "model_name = ['knn', 'logit', 'svm', 'n_bayes', 'random_forest', 'ensemble']\n",
    "\n",
    "# Plot ROC curve for all my models\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for i, model in enumerate(model_list):\n",
    "    if model == 'ensemble':\n",
    "        w1 = 0.10\n",
    "        w2 = 0.80\n",
    "        y_pred = (w1*logistic_grid.best_estimator_.predict_proba(x_norm_test)[:,1] \n",
    "                  + w2*rf_grid.best_estimator_.predict_proba(x_norm_test)[:,1]\n",
    "                  + (1-w1-w2)*gnb_best.predict_proba(x_norm_test)[:,1])\n",
    "    else:\n",
    "        y_pred = list(model.predict_proba(x_norm_test)[:,1])\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = (model_name[i] + ' AUC = %0.4f' % roc_auc))\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Examine the correlation of the model errors\n",
    "\n",
    "knn_error = y_train - knn_grid.predict_proba(x_norm_train)[:,1]\n",
    "logit_error = y_train - logistic_grid.predict_proba(x_norm_train)[:,1]\n",
    "svm_error = y_train - svm_grid.predict_proba(x_norm_train)[:,1]\n",
    "gnb_error = y_train - gnb_best.predict_proba(x_norm_train)[:,1]\n",
    "rf_error = y_train - rf_grid.predict_proba(x_norm_train)[:,1]\n",
    "\n",
    "error_df = pd.DataFrame()\n",
    "error_df['knn'] = knn_error\n",
    "error_df['logit'] = logit_error\n",
    "error_df['svm'] = svm_error\n",
    "error_df['gnb'] = gnb_error\n",
    "error_df['rand_forest'] = rf_error\n",
    "\n",
    "error_df.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
